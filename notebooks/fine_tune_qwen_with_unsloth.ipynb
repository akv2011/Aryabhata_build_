{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 High-Quality JEE Mathematics Problems with Solutions\n",
    "import json\n",
    "\n",
    "jee_math_problems = [\n",
    "    {\n",
    "        \"problem\": \"Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3\",\n",
    "        \"solution\": \"To find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3, I'll use the power rule.\\n\\nStep 1: Apply the power rule to each term\\n- d/dx(x¬≥) = 3x¬≤\\n- d/dx(2x¬≤) = 4x\\n- d/dx(-5x) = -5\\n- d/dx(3) = 0\\n\\nStep 2: Combine the results\\nf'(x) = 3x¬≤ + 4x - 5\\n\\nTherefore, the derivative is f'(x) = 3x¬≤ + 4x - 5\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Solve the quadratic equation: 2x¬≤ - 7x + 3 = 0\",\n",
    "        \"solution\": \"I'll solve 2x¬≤ - 7x + 3 = 0 using the quadratic formula.\\n\\nStep 1: Identify coefficients\\na = 2, b = -7, c = 3\\n\\nStep 2: Apply quadratic formula: x = (-b ¬± ‚àö(b¬≤ - 4ac)) / (2a)\\nx = (7 ¬± ‚àö((-7)¬≤ - 4(2)(3))) / (2(2))\\nx = (7 ¬± ‚àö(49 - 24)) / 4\\nx = (7 ¬± ‚àö25) / 4\\nx = (7 ¬± 5) / 4\\n\\nStep 3: Find both solutions\\nx‚ÇÅ = (7 + 5)/4 = 12/4 = 3\\nx‚ÇÇ = (7 - 5)/4 = 2/4 = 1/2\\n\\nTherefore, x = 3 or x = 1/2\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Find the integral of ‚à´(3x¬≤ + 2x - 1)dx\",\n",
    "        \"solution\": \"To find ‚à´(3x¬≤ + 2x - 1)dx, I'll integrate each term separately.\\n\\nStep 1: Apply the power rule for integration\\n‚à´3x¬≤dx = 3 ¬∑ (x¬≥/3) = x¬≥\\n‚à´2xdx = 2 ¬∑ (x¬≤/2) = x¬≤\\n‚à´(-1)dx = -x\\n\\nStep 2: Combine results and add constant\\n‚à´(3x¬≤ + 2x - 1)dx = x¬≥ + x¬≤ - x + C\\n\\nTherefore, the integral is x¬≥ + x¬≤ - x + C\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Find the limit: lim(x‚Üí2) (x¬≤ - 4)/(x - 2)\",\n",
    "        \"solution\": \"I'll find lim(x‚Üí2) (x¬≤ - 4)/(x - 2) by factoring the numerator.\\n\\nStep 1: Factor the numerator\\nx¬≤ - 4 = (x + 2)(x - 2)\\n\\nStep 2: Substitute and simplify\\nlim(x‚Üí2) (x¬≤ - 4)/(x - 2) = lim(x‚Üí2) (x + 2)(x - 2)/(x - 2)\\n\\nStep 3: Cancel common factors (x ‚â† 2)\\nlim(x‚Üí2) (x + 2) = 2 + 2 = 4\\n\\nTherefore, the limit is 4\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"In triangle ABC, if a = 8, b = 6, and C = 60¬∞, find side c using the law of cosines\",\n",
    "        \"solution\": \"I'll use the law of cosines: c¬≤ = a¬≤ + b¬≤ - 2ab¬∑cos(C)\\n\\nStep 1: Substitute the given values\\na = 8, b = 6, C = 60¬∞\\nc¬≤ = 8¬≤ + 6¬≤ - 2(8)(6)cos(60¬∞)\\n\\nStep 2: Calculate cos(60¬∞) = 1/2\\nc¬≤ = 64 + 36 - 2(8)(6)(1/2)\\nc¬≤ = 100 - 48\\nc¬≤ = 52\\n\\nStep 3: Take the square root\\nc = ‚àö52 = ‚àö(4 √ó 13) = 2‚àö13\\n\\nTherefore, c = 2‚àö13 ‚âà 7.21\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Find the equation of the line passing through points (2, 3) and (5, 9)\",\n",
    "        \"solution\": \"I'll find the equation using the point-slope form.\\n\\nStep 1: Calculate the slope\\nm = (y‚ÇÇ - y‚ÇÅ)/(x‚ÇÇ - x‚ÇÅ) = (9 - 3)/(5 - 2) = 6/3 = 2\\n\\nStep 2: Use point-slope form with point (2, 3)\\ny - y‚ÇÅ = m(x - x‚ÇÅ)\\ny - 3 = 2(x - 2)\\ny - 3 = 2x - 4\\ny = 2x - 1\\n\\nStep 3: Verify with second point (5, 9)\\ny = 2(5) - 1 = 10 - 1 = 9 ‚úì\\n\\nTherefore, the equation is y = 2x - 1\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Solve for x: log‚ÇÇ(x + 3) = 4\",\n",
    "        \"solution\": \"I'll solve log‚ÇÇ(x + 3) = 4 by converting to exponential form.\\n\\nStep 1: Convert logarithmic to exponential form\\nIf log‚ÇÇ(x + 3) = 4, then 2‚Å¥ = x + 3\\n\\nStep 2: Calculate 2‚Å¥\\n2‚Å¥ = 16\\nSo: 16 = x + 3\\n\\nStep 3: Solve for x\\nx = 16 - 3 = 13\\n\\nStep 4: Verify the solution\\nlog‚ÇÇ(13 + 3) = log‚ÇÇ(16) = log‚ÇÇ(2‚Å¥) = 4 ‚úì\\n\\nTherefore, x = 13\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Find the area of a circle with radius r = 5 units\",\n",
    "        \"solution\": \"I'll use the formula A = œÄr¬≤ to find the area.\\n\\nStep 1: Identify the given information\\nRadius r = 5 units\\n\\nStep 2: Apply the area formula\\nA = œÄr¬≤\\nA = œÄ(5)¬≤\\nA = œÄ(25)\\nA = 25œÄ\\n\\nStep 3: Calculate numerical value\\nA = 25œÄ ‚âà 25 √ó 3.14159 ‚âà 78.54 square units\\n\\nTherefore, the area is 25œÄ or approximately 78.54 square units\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If sin(Œ∏) = 3/5 and Œ∏ is in the first quadrant, find cos(Œ∏) and tan(Œ∏)\",\n",
    "        \"solution\": \"I'll use the Pythagorean identity and trigonometric definitions.\\n\\nStep 1: Use sin¬≤(Œ∏) + cos¬≤(Œ∏) = 1\\nsin(Œ∏) = 3/5\\n(3/5)¬≤ + cos¬≤(Œ∏) = 1\\n9/25 + cos¬≤(Œ∏) = 1\\ncos¬≤(Œ∏) = 1 - 9/25 = 16/25\\n\\nStep 2: Find cos(Œ∏)\\nSince Œ∏ is in the first quadrant, cos(Œ∏) > 0\\ncos(Œ∏) = ‚àö(16/25) = 4/5\\n\\nStep 3: Find tan(Œ∏)\\ntan(Œ∏) = sin(Œ∏)/cos(Œ∏) = (3/5)/(4/5) = 3/4\\n\\nTherefore, cos(Œ∏) = 4/5 and tan(Œ∏) = 3/4\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Find the sum of the first 10 terms of the arithmetic sequence: 2, 5, 8, 11, ...\",\n",
    "        \"solution\": \"I'll use the arithmetic sequence sum formula.\\n\\nStep 1: Identify the sequence parameters\\nFirst term a‚ÇÅ = 2\\nCommon difference d = 5 - 2 = 3\\nNumber of terms n = 10\\n\\nStep 2: Find the 10th term\\na‚ÇÅ‚ÇÄ = a‚ÇÅ + (n-1)d = 2 + (10-1)(3) = 2 + 27 = 29\\n\\nStep 3: Use the sum formula\\nS‚Çô = n(a‚ÇÅ + a‚Çô)/2\\nS‚ÇÅ‚ÇÄ = 10(2 + 29)/2 = 10(31)/2 = 155\\n\\nAlternative: S‚Çô = n[2a‚ÇÅ + (n-1)d]/2\\nS‚ÇÅ‚ÇÄ = 10[2(2) + (10-1)(3)]/2 = 10[4 + 27]/2 = 155\\n\\nTherefore, the sum is 155\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to conversational format for training\n",
    "training_data = []\n",
    "for i, item in enumerate(jee_math_problems):\n",
    "    conversation = {\n",
    "        \"id\": f\"jee_synthetic_{i+1}\",\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Solve this JEE mathematics problem step by step:\\n\\n{item['problem']}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": item['solution']\n",
    "            }\n",
    "        ],\n",
    "        \"source\": \"Synthetic JEE Mathematics Dataset\",\n",
    "        \"topic\": \"mathematics\",\n",
    "        \"difficulty\": \"JEE_level\"\n",
    "    }\n",
    "    training_data.append(conversation)\n",
    "\n",
    "print(f\"‚úÖ Created {len(training_data)} high-quality JEE math problems\")\n",
    "print(f\"üìä Average solution length: {sum(len(item['solution']) for item in jee_math_problems) / len(jee_math_problems):.0f} characters\")\n",
    "\n",
    "# Display first example\n",
    "print(f\"\\nüìù Sample Problem:\")\n",
    "print(f\"Problem: {jee_math_problems[0]['problem']}\")\n",
    "print(f\"Solution: {jee_math_problems[0]['solution'][:200]}...\")\n",
    "\n",
    "# Save the dataset\n",
    "import json\n",
    "with open('jee_synthetic_dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüíæ Dataset saved as 'jee_synthetic_dataset.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefeca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and Load Qwen2.5-7B-Instruct Model\n",
    "print(\"üöÄ Installing Unsloth for fast fine-tuning...\")\n",
    "\n",
    "# Install Unsloth (uncomment if running in fresh environment)\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "print(\"üì¶ Loading Qwen2.5-7B-Instruct model with Unsloth...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-7B-Instruct\",  # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token=\"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìä Model dtype: {model.dtype}\")\n",
    "print(f\"üìä Max sequence length: {max_seq_length}\")\n",
    "print(f\"üìä 4-bit quantization: {load_in_4bit}\")\n",
    "\n",
    "# Check model size and parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test the model with a simple math problem\n",
    "print(\"\\nüß™ Testing model before fine-tuning:\")\n",
    "test_prompt = \"\"\"Below is a mathematics problem. Solve it step by step.\n",
    "\n",
    "### Problem:\n",
    "Find the derivative of f(x) = x¬≤ + 3x - 2\n",
    "\n",
    "### Solution:\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7, do_sample=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "print(\"Before fine-tuning response:\")\n",
    "print(response[len(test_prompt):])\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA and Fine-tune on JEE Mathematics\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"‚öôÔ∏è Configuring LoRA for efficient fine-tuning...\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Currently only supports dropout = 0\n",
    "    bias=\"none\",     # Currently only supports bias = \"none\"\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA configuration applied!\")\n",
    "\n",
    "# Prepare the dataset from our synthetic JEE problems\n",
    "def format_prompts(examples):\n",
    "    \"\"\"Format the conversations for training\"\"\"\n",
    "    texts = []\n",
    "    for conversation in examples[\"conversations\"]:\n",
    "        # Create the full conversation\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Convert our training data to HuggingFace dataset\n",
    "print(\"üìä Preparing training dataset...\")\n",
    "dataset = Dataset.from_list(training_data)\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "print(f\"üìä Training examples: {len(dataset)}\")\n",
    "print(f\"üìä Sample formatted text length: {len(dataset[0]['text'])} chars\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=50,  # Short training for demo\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"./qwen_jee_math_lora\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "print(\"üèãÔ∏è Starting fine-tuning...\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Fine-tuning in progress...\")\n",
    "print(\"This will take a few minutes for 50 steps...\")\n",
    "\n",
    "# Start training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"‚úÖ Fine-tuning completed!\")\n",
    "print(f\"üìä Final training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"‚è±Ô∏è Training time: {trainer_stats.metrics['train_runtime']:.1f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"qwen_jee_math_lora\")\n",
    "tokenizer.save_pretrained(\"qwen_jee_math_lora\")\n",
    "\n",
    "print(\"üíæ Model saved to 'qwen_jee_math_lora' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5421ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ead641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"jee_math_dataset/dataset.json\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0baf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting function\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Trainer setup\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
